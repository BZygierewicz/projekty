{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSHqxuyScquv",
    "outputId": "5781b907-ccbe-4246-b975-cab9638dc1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/admin/PycharmProjects/PyTorch_101/.venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/admin/PycharmProjects/PyTorch_101/.venv/lib/python3.13/site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/admin/PycharmProjects/PyTorch_101/.venv/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/admin/PycharmProjects/PyTorch_101/.venv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/admin/PycharmProjects/PyTorch_101/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MaICPnW1d9ci"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "nP4ZwBYmeNph",
    "outputId": "33705f73-4160-4370-bc4a-d2c7047cac89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "5    363122\n",
       "4     80655\n",
       "1     52268\n",
       "3     42640\n",
       "2     29769\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "refn__0afc6B",
    "outputId": "c84df975-e2e8-4e40-92b4-cf0f6b4c15b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(df['Text'].isnull().sum(),\n",
    "        df['Score'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guXiRUeEf4uP",
    "outputId": "efc007f9-2d70-439c-fd54-be578f13e888"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/8g479t651wg0063m05yzr47w0000gn/T/ipykernel_42929/3418805964.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[['Score','Text']].dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df[['Score','Text']].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "iLz82-9LeEok"
   },
   "outputs": [],
   "source": [
    "def convert_score_to_sentiment(score):\n",
    "    if score <= 2:\n",
    "        return 0 # negatywny\n",
    "    elif score == 3:\n",
    "        return 1 # neutralny\n",
    "    else:\n",
    "        return 2 # pozytywny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DSmAO5oje1aB"
   },
   "outputs": [],
   "source": [
    "df['sentiment'] = df['Score'].apply(convert_score_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ex2S4dGQe6gi"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "PH9NkwSzfMYd"
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "qYQA7_FBgoEg"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english',max_features=1000)\n",
    "X = vectorizer.fit_transform(df['Text']).toarray()\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454763,) (113691,) (113691, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y_train.shape, y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpLAXPwfg6XY",
    "outputId": "96e720d2-6ad0-4696-c152-a3ed728b748d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Konwersja danych na tensory Pytorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        # Warstwa wejściowa -> Warstwa ukryta\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Warstwa wyjściowa (3 klasy)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja modelu\n",
    "input_dim = X_train.shape[1]  # Liczba cech (wymiar wektora TF-IDF)\n",
    "hidden_dim = 128  # Liczba jednostek w warstwie ukrytej\n",
    "output_dim = 3    # Liczba klas (pozytywny, neutralny, negatywny)\n",
    "\n",
    "model = SentimentClassifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Ustalamy funkcję straty i optymalizator\n",
    "criterion = nn.CrossEntropyLoss()  # Funkcja straty dla klasyfikacji wieloklasowej\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.7287\n",
      "Epoch [2/150], Loss: 0.7283\n",
      "Epoch [3/150], Loss: 0.7279\n",
      "Epoch [4/150], Loss: 0.7275\n",
      "Epoch [5/150], Loss: 0.7271\n",
      "Epoch [6/150], Loss: 0.7268\n",
      "Epoch [7/150], Loss: 0.7264\n",
      "Epoch [8/150], Loss: 0.7261\n",
      "Epoch [9/150], Loss: 0.7258\n",
      "Epoch [10/150], Loss: 0.7254\n",
      "Epoch [11/150], Loss: 0.7251\n",
      "Epoch [12/150], Loss: 0.7248\n",
      "Epoch [13/150], Loss: 0.7245\n",
      "Epoch [14/150], Loss: 0.7242\n",
      "Epoch [15/150], Loss: 0.7239\n",
      "Epoch [16/150], Loss: 0.7237\n",
      "Epoch [17/150], Loss: 0.7234\n",
      "Epoch [18/150], Loss: 0.7231\n",
      "Epoch [19/150], Loss: 0.7229\n",
      "Epoch [20/150], Loss: 0.7226\n",
      "Epoch [21/150], Loss: 0.7224\n",
      "Epoch [22/150], Loss: 0.7221\n",
      "Epoch [23/150], Loss: 0.7219\n",
      "Epoch [24/150], Loss: 0.7217\n",
      "Epoch [25/150], Loss: 0.7214\n",
      "Epoch [26/150], Loss: 0.7212\n",
      "Epoch [27/150], Loss: 0.7210\n",
      "Epoch [28/150], Loss: 0.7208\n",
      "Epoch [29/150], Loss: 0.7206\n",
      "Epoch [30/150], Loss: 0.7204\n",
      "Epoch [31/150], Loss: 0.7202\n",
      "Epoch [32/150], Loss: 0.7200\n",
      "Epoch [33/150], Loss: 0.7198\n",
      "Epoch [34/150], Loss: 0.7196\n",
      "Epoch [35/150], Loss: 0.7194\n",
      "Epoch [36/150], Loss: 0.7193\n",
      "Epoch [37/150], Loss: 0.7191\n",
      "Epoch [38/150], Loss: 0.7189\n",
      "Epoch [39/150], Loss: 0.7187\n",
      "Epoch [40/150], Loss: 0.7186\n",
      "Epoch [41/150], Loss: 0.7184\n",
      "Epoch [42/150], Loss: 0.7183\n",
      "Epoch [43/150], Loss: 0.7181\n",
      "Epoch [44/150], Loss: 0.7179\n",
      "Epoch [45/150], Loss: 0.7178\n",
      "Epoch [46/150], Loss: 0.7176\n",
      "Epoch [47/150], Loss: 0.7175\n",
      "Epoch [48/150], Loss: 0.7173\n",
      "Epoch [49/150], Loss: 0.7172\n",
      "Epoch [50/150], Loss: 0.7171\n",
      "Epoch [51/150], Loss: 0.7169\n",
      "Epoch [52/150], Loss: 0.7168\n",
      "Epoch [53/150], Loss: 0.7167\n",
      "Epoch [54/150], Loss: 0.7165\n",
      "Epoch [55/150], Loss: 0.7164\n",
      "Epoch [56/150], Loss: 0.7163\n",
      "Epoch [57/150], Loss: 0.7161\n",
      "Epoch [58/150], Loss: 0.7160\n",
      "Epoch [59/150], Loss: 0.7159\n",
      "Epoch [60/150], Loss: 0.7158\n",
      "Epoch [61/150], Loss: 0.7156\n",
      "Epoch [62/150], Loss: 0.7155\n",
      "Epoch [63/150], Loss: 0.7154\n",
      "Epoch [64/150], Loss: 0.7153\n",
      "Epoch [65/150], Loss: 0.7152\n",
      "Epoch [66/150], Loss: 0.7151\n",
      "Epoch [67/150], Loss: 0.7150\n",
      "Epoch [68/150], Loss: 0.7148\n",
      "Epoch [69/150], Loss: 0.7147\n",
      "Epoch [70/150], Loss: 0.7146\n",
      "Epoch [71/150], Loss: 0.7145\n",
      "Epoch [72/150], Loss: 0.7144\n",
      "Epoch [73/150], Loss: 0.7143\n",
      "Epoch [74/150], Loss: 0.7142\n",
      "Epoch [75/150], Loss: 0.7141\n",
      "Epoch [76/150], Loss: 0.7140\n",
      "Epoch [77/150], Loss: 0.7139\n",
      "Epoch [78/150], Loss: 0.7138\n",
      "Epoch [79/150], Loss: 0.7137\n",
      "Epoch [80/150], Loss: 0.7136\n",
      "Epoch [81/150], Loss: 0.7136\n",
      "Epoch [82/150], Loss: 0.7135\n",
      "Epoch [83/150], Loss: 0.7134\n",
      "Epoch [84/150], Loss: 0.7133\n",
      "Epoch [85/150], Loss: 0.7132\n",
      "Epoch [86/150], Loss: 0.7131\n",
      "Epoch [87/150], Loss: 0.7130\n",
      "Epoch [88/150], Loss: 0.7130\n",
      "Epoch [89/150], Loss: 0.7129\n",
      "Epoch [90/150], Loss: 0.7128\n",
      "Epoch [91/150], Loss: 0.7127\n",
      "Epoch [92/150], Loss: 0.7127\n",
      "Epoch [93/150], Loss: 0.7126\n",
      "Epoch [94/150], Loss: 0.7125\n",
      "Epoch [95/150], Loss: 0.7124\n",
      "Epoch [96/150], Loss: 0.7124\n",
      "Epoch [97/150], Loss: 0.7123\n",
      "Epoch [98/150], Loss: 0.7122\n",
      "Epoch [99/150], Loss: 0.7122\n",
      "Epoch [100/150], Loss: 0.7121\n",
      "Epoch [101/150], Loss: 0.7120\n",
      "Epoch [102/150], Loss: 0.7119\n",
      "Epoch [103/150], Loss: 0.7119\n",
      "Epoch [104/150], Loss: 0.7118\n",
      "Epoch [105/150], Loss: 0.7118\n",
      "Epoch [106/150], Loss: 0.7117\n",
      "Epoch [107/150], Loss: 0.7116\n",
      "Epoch [108/150], Loss: 0.7116\n",
      "Epoch [109/150], Loss: 0.7115\n",
      "Epoch [110/150], Loss: 0.7114\n",
      "Epoch [111/150], Loss: 0.7114\n",
      "Epoch [112/150], Loss: 0.7113\n",
      "Epoch [113/150], Loss: 0.7113\n",
      "Epoch [114/150], Loss: 0.7112\n",
      "Epoch [115/150], Loss: 0.7111\n",
      "Epoch [116/150], Loss: 0.7111\n",
      "Epoch [117/150], Loss: 0.7110\n",
      "Epoch [118/150], Loss: 0.7110\n",
      "Epoch [119/150], Loss: 0.7109\n",
      "Epoch [120/150], Loss: 0.7109\n",
      "Epoch [121/150], Loss: 0.7108\n",
      "Epoch [122/150], Loss: 0.7108\n",
      "Epoch [123/150], Loss: 0.7107\n",
      "Epoch [124/150], Loss: 0.7107\n",
      "Epoch [125/150], Loss: 0.7106\n",
      "Epoch [126/150], Loss: 0.7106\n",
      "Epoch [127/150], Loss: 0.7105\n",
      "Epoch [128/150], Loss: 0.7105\n",
      "Epoch [129/150], Loss: 0.7104\n",
      "Epoch [130/150], Loss: 0.7104\n",
      "Epoch [131/150], Loss: 0.7103\n",
      "Epoch [132/150], Loss: 0.7103\n",
      "Epoch [133/150], Loss: 0.7102\n",
      "Epoch [134/150], Loss: 0.7102\n",
      "Epoch [135/150], Loss: 0.7101\n",
      "Epoch [136/150], Loss: 0.7101\n",
      "Epoch [137/150], Loss: 0.7100\n",
      "Epoch [138/150], Loss: 0.7100\n",
      "Epoch [139/150], Loss: 0.7099\n",
      "Epoch [140/150], Loss: 0.7099\n",
      "Epoch [141/150], Loss: 0.7099\n",
      "Epoch [142/150], Loss: 0.7098\n",
      "Epoch [143/150], Loss: 0.7098\n",
      "Epoch [144/150], Loss: 0.7097\n",
      "Epoch [145/150], Loss: 0.7097\n",
      "Epoch [146/150], Loss: 0.7096\n",
      "Epoch [147/150], Loss: 0.7096\n",
      "Epoch [148/150], Loss: 0.7096\n",
      "Epoch [149/150], Loss: 0.7095\n",
      "Epoch [150/150], Loss: 0.7095\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, X_train, y_train, criterion, optimizer, epochs=25):\n",
    "    model.train()  # Ustawiamy model w tryb treningowy\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()  # Zerujemy gradienty\n",
    "        output = model(X_train)  # Przekazujemy dane przez model\n",
    "        loss = criterion(output, y_train)  # Obliczamy stratę\n",
    "        loss.backward()  # Obliczamy gradienty\n",
    "        optimizer.step()  # Wykonujemy krok optymalizacji\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Trenujemy model\n",
    "train_model(model, X_train_tensor, y_train_tensor, criterion, optimizer, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 84.16%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()  # Ustawiamy model w tryb ewaluacji\n",
    "    with torch.no_grad():  # Nie obliczamy gradientów podczas ewaluacji\n",
    "        output = model(X_test)\n",
    "        _, predicted = torch.max(output, 1)  # Wybieramy klasę o najwyższym prawdopodobieństwie\n",
    "        accuracy = accuracy_score(y_test, predicted)\n",
    "        print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Ewaluacja modelu\n",
    "evaluate_model(model, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'sentiment_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the text is: Negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(text, model, vectorizer):\n",
    "    # Wektoryzacja tekstu\n",
    "    text_vector = vectorizer.transform([text]).toarray()  # Konwertujemy na wektor TF-IDF\n",
    "    \n",
    "    # Konwersja na tensor Pytorch\n",
    "    text_tensor = torch.tensor(text_vector, dtype=torch.float32)\n",
    "    \n",
    "    # Przewidywanie za pomocą modelu\n",
    "    model.eval()  # Ustawiamy model w tryb ewaluacji\n",
    "    with torch.no_grad():\n",
    "        output = model(text_tensor)\n",
    "        _, predicted = torch.max(output, 1)  # Wybieramy klasę o najwyższym prawdopodobieństwie\n",
    "    \n",
    "    # Mapowanie numeru klasy na nazwę sentymentu\n",
    "    sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    predicted_sentiment = sentiment_map[predicted.item()]  # Pobieramy wartość z tensora\n",
    "    \n",
    "    return predicted_sentiment\n",
    "\n",
    "# Przykładowe dane do przetestowania\n",
    "text = \"This product sucks\"\n",
    "\n",
    "# Testowanie funkcji na przykładowym tekście\n",
    "predicted_sentiment = predict_sentiment(text, model, vectorizer)\n",
    "print(f'The sentiment of the text is: {predicted_sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
